{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ccf15b7-1cbd-4e79-94c9-9f604a6e7cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "from typing import List, Dict, Union\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "MODELS = {\n",
    "    \"re_ranker\": \"BAAI/bge-reranker-large\",\n",
    "    \"completion\": \"teknium/OpenHermes-2.5-Mistral-7B\",\n",
    "    \"embedding\": \"BAAI/bge-large-en-v1.5\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac7f97-c52d-453a-935e-18e1107c2529",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "- Embedding using **bge-large-en-v1.5**\n",
    "- Saving Embeddings to Pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "983ab9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, sentences: List[Dict], batch_size: int = 64):\n",
    "        for ix in trange(0, len(sentences), batch_size):\n",
    "            tokenized_inputs = self.tokenizer(\n",
    "                sentences[ix:ix + batch_size],\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                model_output = self.model(tokenized_inputs)\n",
    "            for i, ixd in enumerate(range(ix, ix + batch_size)):\n",
    "                sentences[ixd][\"embedding\"] = model_output[i].tolist()\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ff79e2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pinecone'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpinecone\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Dict, Union\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm, trange\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pinecone'"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "from typing import List, Dict, Union\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "\n",
    "class PineconeDB:\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: str,\n",
    "        environment: str,\n",
    "        index_name: str,\n",
    "        namespace: Union[str, None] = None,\n",
    "        batch_size: int = 50,\n",
    "    ):\n",
    "        pinecone.init(api_key=api_key, environment=environment)\n",
    "        self.index = pinecone.Index(index_name)\n",
    "        self.batch_size = batch_size\n",
    "        self.namespace = namespace\n",
    "\n",
    "    def __store__(self, embeddings: List[Dict]):\n",
    "        for ix in trange(0, len(embeddings), self.batch_size, desc=\"Storing Vectors\"):\n",
    "            pvs = []\n",
    "            for ixe, embs in enumerate(embeddings[ix : ix + self.batch_size]):\n",
    "                if len(embs.get(\"embedding\")) > 0:\n",
    "                    e = embs.get(\"embedding\")\n",
    "                    del embs[\"embedding\"]\n",
    "                    pvs.append((str(ix + ixe), e, {**embs}))\n",
    "            if self.namespace:\n",
    "                self.index.upsert(vectors=pvs, namespace=self.namespace)\n",
    "            else:\n",
    "                self.index.upsert(vectors=pvs)\n",
    "\n",
    "    def __call__(self, embeddings: List[Dict]):\n",
    "        self.__store__(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543757f6",
   "metadata": {},
   "source": [
    "## Completion\n",
    "\n",
    "**Completion** and **Function Calling** using **_OpenHeremes-2.5-Mistral-7B_**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "521fcd2d-942d-428c-8a2f-fdffae073e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "\n",
    "\n",
    "class OpenHermesInference:\n",
    "    def __init__(self, model: AutoModelForCausalLM, tokenizer: AutoTokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __inference__(self, messages: List[Dict]):\n",
    "        tokens = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\n",
    "            self.model.device\n",
    "        )\n",
    "        input_size = tokens.numel()\n",
    "        print(\"Input Tokens: \", input_size)\n",
    "        with torch.inference_mode():\n",
    "            generated_tokens = self.model.generate(\n",
    "                tokens,\n",
    "                use_cache=True, do_sample=True, temperature=0.2, top_p=1.0, top_k=0, max_new_tokens=512, eos_token_id=self.tokenizer.eos_token_id, pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        # print(\"Generated Tokens: \", len(generated_tokens.squeeze()))\n",
    "        print(\"Generated New Tokens: \", len(generated_tokens.squeeze()[input_size:]))\n",
    "        # print(\"GENERATED NEW TOKENS: \", generated_tokens.squeeze()[input_size:])\n",
    "        # print(self.tokenizer.decode(generated_tokens.squeeze()))\n",
    "        return self.tokenizer.decode(\n",
    "            generated_tokens.squeeze()[input_size:], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "\n",
    "class FunctionCall(OpenHermesInference):\n",
    "    def __init__(self, model, tokenizer):\n",
    "        super().__init__(self, FunctionCall)\n",
    "        self.system_prompt = \"\"\"You are a helpful assistant with access to the following functions:\n",
    "        \n",
    "            {functions}\n",
    "        \n",
    "            To use these functions respond with:\n",
    "            <multiplefunctions>\n",
    "                <functioncall> {{fn}} </functioncall>\n",
    "                <functioncall> {{fn}} </functioncall>\n",
    "                ...\n",
    "            </multiplefunctions>\n",
    "            \n",
    "            Edge cases you must handle:\n",
    "            - If there are no functions that match the user request, you will respond politely that you cannot help.<|im_end|>\n",
    "\n",
    "            Refer the below provided output example for function calling\n",
    "            Question: What's the weather difference in NY and LA?\n",
    "            <multiplefunctions>\n",
    "                <functioncall> {{\"name\": \"getWeather\", \"parameters\": {{\"city\": \"NY\"}}}} </functioncall>\n",
    "                <functioncall> {{\"name\": \"getWeather\", \"parameters\": {{\"city\": \"LA\"}}}} </functioncall>\n",
    "            </multiplefunctions>\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "    def functionCall(self, messages: List[Dict], functions: List[Dict]):\n",
    "        functions_texts = \"\\n\\n\".join(\n",
    "            [f\"{json.dumps(function)}\" for function in functions]\n",
    "        )\n",
    "        if messages[0].get(\"role\") == \"system\":\n",
    "            new_system_prompt = (\n",
    "                self.system_prompt.format(functions=functions_texts)\n",
    "                + \"\\n\"\n",
    "                + messages[0].get(\"content\")\n",
    "            )\n",
    "            messages[0][\"content\"] = new_system_prompt\n",
    "        else:\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": self.system_prompt.format(functions=functions_texts),\n",
    "                }\n",
    "            ] + messages\n",
    "        output_text = self.__inference__(messages)\n",
    "        return output_text\n",
    "\n",
    "\n",
    "class NormalCompletion(OpenHermesInference):\n",
    "    def __init__(self, model, tokenizer):\n",
    "        super().__init__(self, NormalCompletion)\n",
    "\n",
    "    def normalCompletion(self, messages: List[str]):\n",
    "        output_text = self.__inference__(messages)\n",
    "        return output_text\n",
    "\n",
    "\n",
    "class FunctionExtraction:\n",
    "    def __call__(self, text: str):\n",
    "        completion = text.strip()\n",
    "        pattern = r\"(<multiplefunctions>(.*?)</multiplefunctions>)\"\n",
    "        match = re.search(pattern, completion, re.DOTALL)\n",
    "        if not match:\n",
    "            return None\n",
    "        multiplefn = match.group(1)\n",
    "        root = ET.fromstring(multiplefn)\n",
    "        functions = root.findall(\"functioncall\")\n",
    "        return [json.loads(fn.text) for fn in functions]\n",
    "\n",
    "\n",
    "class Completion(FunctionCall, NormalCompletion, FunctionExtraction):\n",
    "    def __init__(self, model: AutoModelForCausalLM, tokenizer: AutoTokenizer):\n",
    "        super().__init__(self, Completion)\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def chatCompletion(self, messages: List[Dict], functions: Union[None, List] = None):\n",
    "        if functions:\n",
    "            function_call_text = self.functionCall(messages, functions)\n",
    "            # print(function_call_text)\n",
    "            functions = FunctionExtraction()(function_call_text)\n",
    "            # functions = self.extractFunctions(function_call_text)\n",
    "            return functions\n",
    "        else:\n",
    "            return self.normalCompletion(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56ebe9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "728aad1c",
   "metadata": {},
   "source": [
    "## Reranking\n",
    "\n",
    "- Re-ranking usin the **bge-reranker-large**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73cf3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReRanker:\n",
    "\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, query: str, matches: List[str]):\n",
    "        pairs = [[query, match] for match in matches]\n",
    "        with torch.no_grad():\n",
    "            inputs = self.tokenizer(\n",
    "                pairs,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=512,\n",
    "            )\n",
    "            scores = (self.model(**inputs,\n",
    "                                 return_dict=True).logits.view(-1, ).float())\n",
    "        sorted_indices = scores.argsort(descending=True).tolist()\n",
    "        output_ranks = [matches[i] for i in sorted_indices]\n",
    "        return output_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33187472",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPENDENCY_PROMPT = \"\"\"You're a ChatGPT powered query planning agent. Given a user message provide all the question or context dependencies that would need to be addressed to provide a response to the user.\n",
    "You've to break down questions into its dependent queries such that the answers of the dependent query can be used to inform the parent question.\n",
    "You don't need to answer the questions, simply provide the correct sequence of questions to ask and relevant dependencies.\n",
    "Call the function with appropriate data i.e. the dependencies.\n",
    "\"\"\"\n",
    "\n",
    "RAG_ANSWER_PROMPT = \"\"\"You are a ChatGPT powered answering agent. You will be provided with a question and multiple passages in descending order or relevance. Based on those you have to answer the question if possible. If not then you can politely respond saying you cannot help.\"\"\"\n",
    "\n",
    "RAG_FINAL_ANSWER_PROMPT = \"\"\"You are a ChatGPT powered answering agent. You will be provided with a question along with multiple dependencies of that question ans answers for that. Using these you've to provide a coherent answer for the main question if possible. \n",
    "If there is no content the answers of the sub questions that can help answer the main question then respond politely that you cannot help.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce03817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cb013426d24249af43c9e6bb6f7f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6be74e805d4ad1ae5d3a1b73f836a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe99d01d2f904644989073a179f827af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ecd329759204abab02db8b47f779eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/101 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a8115c574a4b938f28034833b71c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2305d8ce1344175adaf909b9a8e791e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6a0baf6c7e40ebad0ec59c150770f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac83e1d64a9147aab7dc8d0e6fa6aff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa228f276ad745388aefc6062fa5177e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d19ced74b14c51bd38737c89f3d13a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5128d9628c476ebe6615193345a449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/120 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# embedding_tokenizer = AutoTokenizer.from_pretrained(MODELS[\"embedding\"])\n",
    "# embedding_model = AutoModel.from_pretrained(\n",
    "#     MODELS[\"embedding\"], device_map=\"auto\"\n",
    "# ).eval()\n",
    "\n",
    "# rerank_tokenizer = AutoTokenizer.from_pretrained(MODELS[\"re_ranker\"])\n",
    "# rerank_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     MODELS[\"re_ranker\"]\n",
    "# ).eval()\n",
    "\n",
    "completion_tokenizer = AutoTokenizer.from_pretrained(MODELS[\"completion\"])\n",
    "completion_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODELS[\"completion\"], torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743e4e89-0771-41ce-8409-9e9b05a772a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "335ac96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_obj = Embedding(embedding_model, embedding_tokenizer)\n",
    "\n",
    "# reranking_obj = ReRanker(rerank_model, rerank_tokenizer)\n",
    "\n",
    "completion_obj = Completion(completion_model, completion_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69ecd98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_194/3132235189.py:27: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  \"parameters\": Dependencies.schema()\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "class QueryDependency(BaseModel):\n",
    "    id: int = Field(..., description=\"Unique Integer Id for the Query\")\n",
    "    question: str = Field(\n",
    "        ...,\n",
    "        description=\n",
    "        \"Question we want to ask to get a better context or more background about the main question.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class Dependencies(BaseModel):\n",
    "    dependencies: List[QueryDependency] = Field(\n",
    "        ...,\n",
    "        description=\n",
    "        \"A list of query dependencies in the correct sequence to fetch more background information about the main question.\",\n",
    "    )\n",
    "    # questions: List[str]\n",
    "\n",
    "\n",
    "functions = [{\n",
    "    \"name\": \"dependencyPlanning\",\n",
    "    \"description\":\n",
    "    \"Plan a sequential list of all the sub-questions that once answered can provide more background to answer the main question.\",\n",
    "    \"parameters\": Dependencies.schema()\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f9f26a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tokens:  574\n",
      "Generated New Tokens:  97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'dependencyPlanning',\n",
       "  'parameters': {'dependencies': [{'id': 1,\n",
       "     'question': 'What is the capital of France?'},\n",
       "    {'id': 2, 'question': 'What is the capital of the United Kingdom?'},\n",
       "    {'id': 3,\n",
       "     'question': 'What is the distance between the two capital cities?'}]}}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what's the distance between the capital of France and capital of United Kingdom?\"\n",
    "completion_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"{DEPENDENCY_PROMPT}\n",
    "\n",
    "        Question: {question}\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\"role\": \"assistant\", \"content\": \"\"}\n",
    "]\n",
    "function_calls = completion_obj.chatCompletion(completion_messages, functions)\n",
    "function_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2491888e-f232-4942-b26d-96aac056c41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'dependencyPlanning', 'parameters': {'dependencies': [{'id': 1, 'question': 'What is the capital of France?'}, {'id': 2, 'question': 'What is the capital of the United Kingdom?'}, {'id': 3, 'question': 'What is the distance between the two capital cities?'}]}}]\n"
     ]
    }
   ],
   "source": [
    "print(function_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ef1f402-6745-4213-a189-cf845aa51f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tokens:  574\n",
      "Generated New Tokens:  106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'dependencyPlanning',\n",
       "  'parameters': {'dependencies': [{'id': 1, 'question': 'What is GPT-4?'},\n",
       "    {'id': 2, 'question': 'What is Mistral-7B?'},\n",
       "    {'id': 3,\n",
       "     'question': 'What are the benchmarks used to compare AI models?'}]}}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"provide comparison between GPT-4 and Mistral-7B models benchmarks\"\n",
    "completion_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"{DEPENDENCY_PROMPT}\n",
    "\n",
    "        Question: {question}\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\"role\": \"assistant\", \"content\": \"\"}\n",
    "]\n",
    "function_calls = completion_obj.chatCompletion(completion_messages, functions)\n",
    "function_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57c7fca6-e4ee-4bc4-b07f-c3b118443064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tokens:  570\n",
      "Generated New Tokens:  180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'dependencyPlanning',\n",
       "  'parameters': {'dependencies': [{'id': 1,\n",
       "     'question': 'What are the features of iPhone 14 Pro?'},\n",
       "    {'id': 2, 'question': 'What are the features of iPhone 15 Pro?'}]}}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"compare iPhone 14 pro with iPhone 15 pro\"\n",
    "completion_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"{DEPENDENCY_PROMPT}\n",
    "\n",
    "        Question: {question}\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\"role\": \"assistant\", \"content\": \"\"}\n",
    "]\n",
    "function_calls = completion_obj.chatCompletion(completion_messages, functions)\n",
    "function_calls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
