{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c5944f4-71f8-48ca-b709-88266f47fcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import List, Dict, Union\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "MODEL = \"teknium/OpenHermes-2.5-Mistral-7B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ca2c6a9-8e2e-49ec-b8c1-914df61d91c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "\n",
    "\n",
    "class OpenHermesInference:\n",
    "\n",
    "    def __init__(self, model: AutoModelForCausalLM, tokenizer: AutoTokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __inference__(self, messages: List[Dict]):\n",
    "        tokens = self.tokenizer.apply_chat_template(messages,\n",
    "                                                    return_tensors=\"pt\").to(\n",
    "                                                        self.model.device)\n",
    "        input_size = tokens.numel()\n",
    "        print(\"Input Tokens: \", input_size)\n",
    "        with torch.inference_mode():\n",
    "            generated_tokens = self.model.generate(\n",
    "                tokens,\n",
    "                use_cache=True,\n",
    "                do_sample=True,\n",
    "                temperature=0.2,\n",
    "                top_p=1.0,\n",
    "                top_k=0,\n",
    "                max_new_tokens=512,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "        print(\"Generated New Tokens: \",\n",
    "              len(generated_tokens.squeeze()[input_size:]))\n",
    "        return self.tokenizer.decode(generated_tokens.squeeze()[input_size:],\n",
    "                                     skip_special_tokens=True)\n",
    "\n",
    "\n",
    "class FunctionCall(OpenHermesInference):\n",
    "\n",
    "    def __init__(self, model, tokenizer):\n",
    "        super().__init__(self, FunctionCall)\n",
    "        self.system_prompt = \"\"\"You are a helpful assistant with access to the following functions:\n",
    "        \n",
    "            {functions}\n",
    "        \n",
    "            To use these functions respond with:\n",
    "            <multiplefunctions>\n",
    "                <functioncall> {{fn}} </functioncall>\n",
    "                <functioncall> {{fn}} </functioncall>\n",
    "                ...\n",
    "            </multiplefunctions>\n",
    "            \n",
    "            Edge cases you must handle:\n",
    "            - If there are no functions that match the user request, you will respond politely that you cannot help.<|im_end|>\n",
    "\n",
    "            Refer the below provided output example for function calling\n",
    "            Question: What's the weather difference in NY and LA?\n",
    "            <multiplefunctions>\n",
    "                <functioncall> {{\"name\": \"getWeather\", \"parameters\": {{\"city\": \"NY\"}}}} </functioncall>\n",
    "                <functioncall> {{\"name\": \"getWeather\", \"parameters\": {{\"city\": \"LA\"}}}} </functioncall>\n",
    "            </multiplefunctions>\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "    def functionCall(self, messages: List[Dict], functions: List[Dict]):\n",
    "        functions_texts = \"\\n\\n\".join(\n",
    "            [f\"{json.dumps(function)}\" for function in functions])\n",
    "        if messages[0].get(\"role\") == \"system\":\n",
    "            new_system_prompt = (\n",
    "                self.system_prompt.format(functions=functions_texts) + \"\\n\" +\n",
    "                messages[0].get(\"content\"))\n",
    "            messages[0][\"content\"] = new_system_prompt\n",
    "        else:\n",
    "            messages = [{\n",
    "                \"role\":\n",
    "                \"system\",\n",
    "                \"content\":\n",
    "                self.system_prompt.format(functions=functions_texts),\n",
    "            }] + messages\n",
    "        output_text = self.__inference__(messages)\n",
    "        return output_text\n",
    "\n",
    "\n",
    "class NormalCompletion(OpenHermesInference):\n",
    "\n",
    "    def __init__(self, model, tokenizer):\n",
    "        super().__init__(self, NormalCompletion)\n",
    "\n",
    "    def normalCompletion(self, messages: List[str]):\n",
    "        output_text = self.__inference__(messages)\n",
    "        return output_text\n",
    "\n",
    "\n",
    "class FunctionExtraction:\n",
    "\n",
    "    def __call__(self, text: str):\n",
    "        completion = text.strip()\n",
    "        pattern = r\"(<multiplefunctions>(.*?)</multiplefunctions>)\"\n",
    "        match = re.search(pattern, completion, re.DOTALL)\n",
    "        if not match:\n",
    "            return None\n",
    "        multiplefn = match.group(1)\n",
    "        root = ET.fromstring(multiplefn)\n",
    "        functions = root.findall(\"functioncall\")\n",
    "        return [json.loads(fn.text) for fn in functions]\n",
    "\n",
    "\n",
    "class Completion(FunctionCall, NormalCompletion, FunctionExtraction):\n",
    "\n",
    "    def __init__(self, model: AutoModelForCausalLM, tokenizer: AutoTokenizer):\n",
    "        super().__init__(self, Completion)\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def chatCompletion(self,\n",
    "                       messages: List[Dict],\n",
    "                       functions: Union[None, List] = None):\n",
    "        if functions:\n",
    "            function_call_text = self.functionCall(messages, functions)\n",
    "            functions = FunctionExtraction()(function_call_text)\n",
    "            return functions\n",
    "        else:\n",
    "            return self.normalCompletion(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a774e81b-f146-4a57-ba44-b2d99aa28459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564c660ce5824cf1b7429a554a84a31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "completion_tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "completion_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77d35da5-6f99-49ae-a0bc-65c134adffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_obj = Completion(completion_model, completion_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c4e790b-b7a3-4438-bed9-7c89f1368d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_484/580731889.py:26: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  \"parameters\": Dependencies.schema()\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "class QueryDependency(BaseModel):\n",
    "    id: int = Field(..., description=\"Unique Integer Id for the Query\")\n",
    "    question: str = Field(\n",
    "        ...,\n",
    "        description=\n",
    "        \"Question we want to ask to get a better context or more background about the main question.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class Dependencies(BaseModel):\n",
    "    dependencies: List[QueryDependency] = Field(\n",
    "        ...,\n",
    "        description=\n",
    "        \"A list of query dependencies in the correct sequence to fetch more background information about the main question.\",\n",
    "    )\n",
    "\n",
    "\n",
    "functions = [{\n",
    "    \"name\": \"dependencyPlanning\",\n",
    "    \"description\":\n",
    "    \"Plan a sequential list of all the sub-questions that once answered can provide more background to answer the main question.\",\n",
    "    \"parameters\": Dependencies.schema(),\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1442854d-d5c7-4582-bff1-a7304f4bb239",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPENDENCY_PROMPT = \"\"\"You're a ChatGPT powered query planning agent. Given a user message provide all the question or context dependencies that would need to be addressed to provide a response to the user.\n",
    "You've to break down questions into its dependent queries such that the answers of the dependent query can be used to inform the parent question.\n",
    "You don't need to answer the questions, simply provide the correct sequence of questions to ask and relevant dependencies.\n",
    "Call the function with appropriate data i.e. the dependencies.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "019dd50c-4deb-4a39-9ab2-0cd6cae50450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tokens:  574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated New Tokens:  85\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'dependencyPlanning',\n",
       "  'parameters': {'dependencies': [{'id': 1,\n",
       "     'question': 'What is the capital of France?'},\n",
       "    {'id': 2, 'question': 'What is the capital of the United Kingdom?'}]}}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = (\n",
    "    \"what's the distance between the capital of France and capital of United Kingdom?\"\n",
    ")\n",
    "completion_messages = [\n",
    "    {\n",
    "        \"role\":\n",
    "        \"user\",\n",
    "        \"content\":\n",
    "        f\"\"\"{DEPENDENCY_PROMPT}\n",
    "\n",
    "        Question: {question}\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"\"\n",
    "    },\n",
    "]\n",
    "function_calls = completion_obj.chatCompletion(completion_messages, functions)\n",
    "function_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7516554-f1fc-4ea5-8b1a-4f843777e44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tokens:  574\n",
      "Generated New Tokens:  131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'dependencyPlanning',\n",
       "  'parameters': {'dependencies': [{'id': 1,\n",
       "     'question': 'What are the key features and capabilities of GPT-4?'},\n",
       "    {'id': 2,\n",
       "     'question': 'What are the key features and capabilities of Mistral-7B?'},\n",
       "    {'id': 3,\n",
       "     'question': 'How do the benchmarks of GPT-4 and Mistral-7B compare in terms of performance and accuracy?'}]}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"provide comparison between GPT-4 and Mistral-7B models benchmarks\"\n",
    "completion_messages = [\n",
    "    {\n",
    "        \"role\":\n",
    "        \"user\",\n",
    "        \"content\":\n",
    "        f\"\"\"{DEPENDENCY_PROMPT}\n",
    "\n",
    "        Question: {question}\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"\"\n",
    "    },\n",
    "]\n",
    "function_calls = completion_obj.chatCompletion(completion_messages, functions)\n",
    "function_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0c6e1fd-b19f-4d8d-ad02-9975bbfff926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tokens:  570\n",
      "Generated New Tokens:  93\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'dependencyPlanning',\n",
       "  'parameters': {'dependencies': [{'id': 1,\n",
       "     'question': 'What are the key features of iPhone 14 Pro?'},\n",
       "    {'id': 2, 'question': 'What are the key features of iPhone 15 Pro?'}]}}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"compare iPhone 14 pro with iPhone 15 pro\"\n",
    "completion_messages = [\n",
    "    {\n",
    "        \"role\":\n",
    "        \"user\",\n",
    "        \"content\":\n",
    "        f\"\"\"{DEPENDENCY_PROMPT}\n",
    "\n",
    "        Question: {question}\n",
    "        \"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"\"\n",
    "    },\n",
    "]\n",
    "function_calls = completion_obj.chatCompletion(completion_messages, functions)\n",
    "function_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29ee817-4e14-4ad1-9fc6-bc42026f32ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
